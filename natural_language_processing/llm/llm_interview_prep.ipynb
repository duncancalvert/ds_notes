{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Interview Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² & ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´\n",
    "\n",
    "### Transformer Architecture (attention mechanisms\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "### Training Objectives (next token prediction)\n",
    "\n",
    "### Context Window and Position Embeddings\n",
    "\n",
    "### Tokenization Strategies\n",
    "\n",
    "### Model Scaling Laws\n",
    "\n",
    "### Parameter Efficient Fine-tuning (LoRA, QLoRA, Prefix Tuning)\n",
    "\n",
    "### Distillation\n",
    "\n",
    "What is knowledge distillation?\n",
    "\n",
    "When did knowledge distillation appear as a technique?\n",
    "* The ideas behind knowledge distillation (KD) date back to 2006, when BucilÄƒ, Caruana, and Niculescu-Mizil in their work â€œModel Compressionâ€ showed that an ensemble of models could be compressed into a single smaller model without much loss in accuracy. They demonstrated that a cumbersome model (like an ensemble) could be effectively replaced by a lean model that was easier to deploy.\n",
    "* Later in 2015, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean coined the term â€œdistillationâ€ in their â€œDistilling the Knowledge in a Neural Networkâ€ paper. This term was referred to the process of transferring knowledge from a large, complex AI model or ensemble to a smaller, faster AI model, called the distilled modelâ€‹. \n",
    "    * Instead of just training the smaller model on correct answers, researchers proposed to give it the probability distribution from the large model. \n",
    "    * This helps the smaller model learn not just what the right answer is, but also how confident the big model is about each option. This training concept is closely connected to the softmax function.\n",
    "\n",
    "Types of knowledge distillation\n",
    "\n",
    "Improved algorithms\n",
    "\n",
    "Distillation scaling laws\n",
    "\n",
    "Benefits\n",
    "\n",
    "Not without limitations\n",
    "\n",
    "Real-world effective use cases (why OpenAI got mad at DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹\n",
    "\n",
    "### Temperature and Top-p Sampling\n",
    "\n",
    "### Prompt Engineering Techniques\n",
    "\n",
    "### Few-shot Learning\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "### Chain-of-Thought Prompting\n",
    "\n",
    "### Hallucination Mitigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—Ÿğ—Ÿğ—  ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»\n",
    "\n",
    "### Metrics \n",
    "1. Precision@K\n",
    "2. Apology Rate\n",
    "3. No Response Rate\n",
    "4. Perplexity\n",
    "5. ROUGE Scores\n",
    "6. BLEU Scores\n",
    "7. Intelligence\n",
    "8. Relevance\n",
    "\n",
    "### LLM-as-a-judge\n",
    "\n",
    "### Human Evaluation Methods\n",
    "\n",
    "### Evaluation Data Sets\n",
    "* Benchmark Datasets (MMLU, BigBench, HumanEval)\n",
    "\n",
    "### A/B Testing Between Chatbots\n",
    "\n",
    "### Bias Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» & ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜\n",
    "\n",
    "### Quantization Techniques (4-bit, 8-bit)\n",
    "\n",
    "### Model Distillation\n",
    "\n",
    "### Prompt Caching\n",
    "\n",
    "### Model Merging\n",
    "\n",
    "### Inference Optimization\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "### Latency Management\n",
    "\n",
    "### Cost Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğ—¦ğ—®ğ—³ğ—²ğ˜ğ˜† & ğ—˜ğ˜ğ—µğ—¶ğ—°ğ˜€\n",
    "\n",
    "### Content Filtering\n",
    "\n",
    "### Output Sanitization\n",
    "\n",
    "### Jailbreak Prevention\n",
    "\n",
    "### Data Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Product Sense - Good vs. Bad LLM Use Cases\n",
    "\n",
    "### Good LLM Use Cases\n",
    "\n",
    "### Bad LLM Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
