{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Interview Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² & ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´\n",
    "\n",
    "### Transformer Architecture (attention mechanisms)\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "### Training Objectives (next token prediction)\n",
    "\n",
    "### Context Window and Position Embeddings\n",
    "\n",
    "### Tokenization Strategies\n",
    "\n",
    "### Model Scaling Laws\n",
    "\n",
    "### Parameter Efficient Fine-tuning (LoRA, QLoRA, Prefix Tuning)\n",
    "\n",
    "### Distillation\n",
    "\n",
    "What is knowledge distillation?\n",
    "\n",
    "When did knowledge distillation appear as a technique?\n",
    "* The ideas behind knowledge distillation (KD) date back to 2006, when BucilÄƒ, Caruana, and Niculescu-Mizil in their work â€œModel Compressionâ€ showed that an ensemble of models could be compressed into a single smaller model without much loss in accuracy. They demonstrated that a cumbersome model (like an ensemble) could be effectively replaced by a lean model that was easier to deploy.\n",
    "* Later in 2015, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean coined the term â€œdistillationâ€ in their â€œDistilling the Knowledge in a Neural Networkâ€ paper. This term was referred to the process of transferring knowledge from a large, complex AI model or ensemble to a smaller, faster AI model, called the distilled modelâ€‹. \n",
    "    * Instead of just training the smaller model on correct answers, researchers proposed to give it the probability distribution from the large model. \n",
    "    * This helps the smaller model learn not just what the right answer is, but also how confident the big model is about each option. This training concept is closely connected to the softmax function.\n",
    "\n",
    "Types of knowledge distillation\n",
    "\n",
    "Improved algorithms\n",
    "\n",
    "Distillation scaling laws\n",
    "\n",
    "Benefits\n",
    "\n",
    "Not without limitations\n",
    "\n",
    "Real-world effective use cases (why OpenAI got mad at DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹\n",
    "\n",
    "### Temperature and Top-p Sampling\n",
    "\n",
    "### Prompt Engineering Techniques\n",
    "\n",
    "### Few-shot Learning\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "### Chain-of-Thought Prompting\n",
    "\n",
    "### Hallucination Mitigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—Ÿğ—Ÿğ—  ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—» - Metrics \n",
    "\n",
    "### Precision@K\n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It is the ratio of correctly identified relevant items within the total recommended items (the K-long list). As such, it answers the question, out of the top-K items suggested, how many are actually relevant to the user? \n",
    "* K is the cut-off threshold that you choose to limit your evaluation to and it's dependent on the expectation of how many items your user is likely to interact with.\n",
    "* Relevance is a binary label that is specific to your use case. For example, you might measure an item as relevant if a user clicks on it, adds it to a shopping cart, or purchases it.\n",
    "\n",
    "$\\text{Precision@K} = \\frac{\\text{Number of relevant items in K}}{Total number of items in K}$\n",
    "\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### Recall@K \n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It measures the share of revelant items captured within the top K positions\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### F-Score@K\n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It provides a balanced measure of Precision@K and Recall@K\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### Apology Rate\n",
    "\n",
    "### No Response Rate\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "### ROUGE Scores\n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "### Intelligence\n",
    "\n",
    "### Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM Eval Techniques\n",
    "\n",
    "### LLM-as-a-judge\n",
    "\n",
    "### Human Evaluation Methods\n",
    "\n",
    "### Evaluation Data Sets\n",
    "* Benchmark Datasets (MMLU, BigBench, HumanEval)\n",
    "\n",
    "### A/B Testing Between Chatbots\n",
    "\n",
    "### Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» & ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜\n",
    "\n",
    "### Quantization Techniques (4-bit, 8-bit)\n",
    "\n",
    "### Model Distillation\n",
    "\n",
    "### Prompt Caching\n",
    "\n",
    "### Model Merging\n",
    "\n",
    "### Inference Optimization\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "### Latency Management\n",
    "\n",
    "### Cost Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğ—¦ğ—®ğ—³ğ—²ğ˜ğ˜† & ğ—˜ğ˜ğ—µğ—¶ğ—°ğ˜€\n",
    "\n",
    "### Content Filtering\n",
    "\n",
    "### Output Sanitization\n",
    "\n",
    "### Jailbreak Prevention\n",
    "\n",
    "### Data Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Product Sense - Good vs. Bad LLM Use Cases\n",
    "\n",
    "### Good LLM Use Cases\n",
    "\n",
    "### Bad LLM Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
