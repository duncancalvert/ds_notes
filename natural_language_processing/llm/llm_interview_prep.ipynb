{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Interview Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 𝗔𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲 & 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴\n",
    "\n",
    "### Transformer Architecture (attention mechanisms)\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "### Training Objectives (next token prediction)\n",
    "\n",
    "### Context Window and Position Embeddings\n",
    "\n",
    "### Tokenization Strategies\n",
    "\n",
    "### Model Scaling Laws\n",
    "\n",
    "### Parameter Efficient Fine-tuning (LoRA, QLoRA, Prefix Tuning)\n",
    "\n",
    "### Distillation\n",
    "\n",
    "What is knowledge distillation?\n",
    "\n",
    "When did knowledge distillation appear as a technique?\n",
    "* The ideas behind knowledge distillation (KD) date back to 2006, when Bucilă, Caruana, and Niculescu-Mizil in their work “Model Compression” showed that an ensemble of models could be compressed into a single smaller model without much loss in accuracy. They demonstrated that a cumbersome model (like an ensemble) could be effectively replaced by a lean model that was easier to deploy.\n",
    "* Later in 2015, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean coined the term “distillation” in their “Distilling the Knowledge in a Neural Network” paper. This term was referred to the process of transferring knowledge from a large, complex AI model or ensemble to a smaller, faster AI model, called the distilled model​. \n",
    "    * Instead of just training the smaller model on correct answers, researchers proposed to give it the probability distribution from the large model. \n",
    "    * This helps the smaller model learn not just what the right answer is, but also how confident the big model is about each option. This training concept is closely connected to the softmax function.\n",
    "\n",
    "Types of knowledge distillation\n",
    "\n",
    "Improved algorithms\n",
    "\n",
    "Distillation scaling laws\n",
    "\n",
    "Benefits\n",
    "\n",
    "Not without limitations\n",
    "\n",
    "Real-world effective use cases (why OpenAI got mad at DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 𝗖𝗼𝗻𝘁𝗿𝗼𝗹\n",
    "\n",
    "### Temperature and Top-p Sampling\n",
    "\n",
    "### Prompt Engineering Techniques\n",
    "\n",
    "### Few-shot Learning\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "### Chain-of-Thought Prompting\n",
    "\n",
    "### Hallucination Mitigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗟𝗟𝗠 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻 - Metrics \n",
    "\n",
    "### Precision@K\n",
    "\n",
    "<ins>Precision@K</ins>: A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It is the ratio of correctly identified relevant items within the total recommended items (the K-long list). As such, it answers the question, out of the top-K items suggested, how many are actually relevant to the user?\n",
    "* K is the cut-off threshold that you choose to limit your evaluation to and it's dependent on the expectation of how many items your user is likely to interact with.\n",
    "* Relevance is a binary label that is specific to your use case. For example, you might measure an item as relevant if a user clicks on it, adds it to a shopping cart, or purchases it.\n",
    "* <ins>Used in</ins>: ranking algorithms (search systems, RAG)\n",
    "* <ins>Range</ins>: 0-1, a higher value means better performance\n",
    "* <ins>Limitations</ins>:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "<ins>Equation</ins>: $\\text{Precision@K} = \\frac{\\text{Number of relevant items in K}}{\\text{Total number of items in K}}$\n",
    "\n",
    "\n",
    "### Recall@K \n",
    "\n",
    "<ins>Recall@K</ins>: A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It measures the share of revelant items captured within the top K positions\n",
    "* <ins>Used in</ins>: ranking algorithms (search systems, RAG)\n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### F-Score@K\n",
    "\n",
    "<ins>F-Score@K</ins>: A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It provides a balanced measure of Precision@K and Recall@K\n",
    "* <ins>Used in</ins>: ranking algorithms (search systems, RAG)\n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: 0-1, a higher value means better performance\n",
    "* <ins>Limitations</ins>:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### Apology Rate\n",
    "\n",
    "<ins>Apology Rate</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### No Response Rate\n",
    "\n",
    "<ins>No Response Rate</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "<ins>Perplexity</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### ROUGE Scores\n",
    "\n",
    "<ins>ROUGE Scores</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "<ins>BLEU Score</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### Intelligence\n",
    "\n",
    "<ins>Intelligence</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### Relevance\n",
    "\n",
    "<ins>Relevance</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark Metrics\n",
    "\n",
    "### ELO\n",
    "\n",
    "<ins>ELO</ins>: \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Equation</ins>: \n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM Eval Techniques\n",
    "\n",
    "### LLM-as-a-judge\n",
    "\n",
    "### Human Evaluation Methods\n",
    "\n",
    "### Evaluation Data Sets\n",
    "* Benchmark Datasets (MMLU, BigBench, HumanEval)\n",
    "\n",
    "### A/B Testing Between Chatbots\n",
    "\n",
    "### Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LLM Interpretability\n",
    "\n",
    "### Dictionary Learning\n",
    "\n",
    "<ins>Dictionary Learning</ins>: a technique developed by Anthropic that helps uncover millions of neuron patterns or “features” and match them to human concepts. Think of it as building a rough glossary for the brain of an LLM.\n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Citations</ins>: \n",
    "    * [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html?utm_source=www.turingpost.com&utm_medium=newsletter&utm_campaign=gemini-is-rising-while-anthropic-works-on-opening-the-black-box-of-ai&_bhlid=2e798d9d7f3d0fa403e5dd525162c467e41be7b2)\n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### Monosemanticity\n",
    "\n",
    "<ins>Dictionary Learning</ins>: the idea that a single neuron pattern might align with a single meaning. A clean signal, not a messy blend. Theoretical work laid out by Anthropic's interpretability team.\n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Citations</ins>: \n",
    "    * [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning?utm_source=www.turingpost.com&utm_medium=newsletter&utm_campaign=gemini-is-rising-while-anthropic-works-on-opening-the-black-box-of-ai&_bhlid=8606cb7ea3b2e400fc2c44136f3dc1a6f8b28041)\n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:\n",
    "\n",
    "### Attributional Graphs\n",
    "\n",
    "<ins>Dictionary Learning</ins>: A technique used by Anthropic's interpretability team to trace how Claude 3.5 Haiku reasons across multiple steps – writing poems, diagnosing patients, even planning ahead. The findings make one thing clear, these models aren’t just completing sentences. They’re constructing thoughts. \n",
    "* <ins>Used in</ins>: \n",
    "* <ins>Citations</ins>: \n",
    "    * [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html?utm_source=www.turingpost.com&utm_medium=newsletter&utm_campaign=gemini-is-rising-while-anthropic-works-on-opening-the-black-box-of-ai&_bhlid=357b4f39d91905c184ff9190a3bcc3eff0d7ea32)\n",
    "* <ins>Range</ins>: \n",
    "* <ins>Limitations</ins>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗮𝘁𝗶𝗼𝗻 & 𝗗𝗲𝗽𝗹𝗼𝘆𝗺𝗲𝗻𝘁\n",
    "\n",
    "### Quantization Techniques (4-bit, 8-bit)\n",
    "\n",
    "### Model Distillation\n",
    "\n",
    "### Prompt Caching\n",
    "\n",
    "### Model Merging\n",
    "\n",
    "### Inference Optimization\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "### Latency Management\n",
    "\n",
    "### Cost Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 𝗦𝗮𝗳𝗲𝘁𝘆 & 𝗘𝘁𝗵𝗶𝗰𝘀\n",
    "\n",
    "### Content Filtering\n",
    "\n",
    "### Output Sanitization\n",
    "\n",
    "### Jailbreak Prevention\n",
    "\n",
    "### Data Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "<ins>RAG</ins>: an architectural framework, introduced by Meta AI Research, that enhances LLMs by incorporating external data retrieval mechanisms. This integration allows LLMs to access real-time, relevant information, thereby addressing the limitations of traditional generative models that rely solely on static training data. By retrieving pertinent documents or data points in response to specific queries, RAG ensures that the generated outputs are not only contextually appropriate but also factually accurate, significantly reducing the incidence of outdated or erroneous information. This capability is particularly beneficial in applications such as customer support and knowledge management, where timely and precise responses are critical. \n",
    "\n",
    "The primary methods employed in RAG involve a two-stage process: \n",
    "1. Retrieving relevant information from a curated set of external sources.\n",
    "2. Utilizing this information to inform the generation of responses. \n",
    "\n",
    "This dual approach allows RAG to dynamically augment the generative capabilities of LLMs with up-to-date context, enhancing their performance across various tasks. Techniques such as vector-based retrieval and query expansion are commonly used to improve the relevance and accuracy of the retrieved information. Furthermore, RAG systems can be designed to include mechanisms for citation and source attribution, enabling users to verify the accuracy of the generated content and fostering trust in AI outputs.\n",
    "\n",
    "### Issues with RAG\n",
    "Despite its advantages, implementing RAG poses several challenges that organizations must navigate. One significant hurdle is the complexity of integrating retrieval systems with generative models, which requires specialized knowledge in both natural language processing and information retrieval. Additionally, the effectiveness of a RAG system is heavily dependent on the quality and reliability of the external data sources it utilizes; poor-quality data can lead to misleading outputs or propagate inaccuracies. Latency issues can also arise during retrieval operations, particularly when accessing large datasets or multiple sources simultaneously, potentially impacting user experience in time-sensitive applications.\n",
    "\n",
    "### RAG Implementation Process (2 Step)\n",
    "\n",
    "#### 1. Retrieval QA Chain: Ingestion\n",
    "1. Ingest documents\n",
    "2. Split documents into chunks\n",
    "3. Convert chunks into vectors via an embedding model\n",
    "4. Create an index of vectors (Vector Store)\n",
    "\n",
    "#### 2. Retrieval QA Chain: Query Time\n",
    "1. User queries model\n",
    "2. Convert user query into vector using embedding model\n",
    "3. Conduct similarity search using query vector and vector store\n",
    "4. Retrieve top-k relevant documents\n",
    "5. LLM generates response using documents as additional context\n",
    "\n",
    "### Secondary Concepts\n",
    "* REALM Technique (RAG during pretraining)\n",
    "* Speculative RAG (Google Research)\n",
    "* Corrective RAG\n",
    "* Self-RAG (Allen AI)\n",
    "* Fusion-RAG\n",
    "* Graph RAG (Microsoft Research)\n",
    "* Hypothetical Document Embeddings (HyDE) - the technique that inspired RAG\n",
    "* RAG vs. Fine-Tuning (UC Berkeley paper, retrieval augmented fine tuning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multimodal Retrieval Augmented Generation (MLLM RAG)\n",
    "\n",
    "### Retrieval QA Chain: Ingestion\n",
    "Same flow as standard RAG but for images do the following:\n",
    "* Extract images from files\n",
    "* Send images to MLLM to generate an image description\n",
    "* Create embeddings of image with multimodal embeddings\n",
    "* Create embeddings of text description from MLLMs using text embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agents\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "* [A Deep Dive Into MCP and the Future of AI Tooling](https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Product Sense - Good vs. Bad LLM Use Cases\n",
    "\n",
    "### Good LLM Use Cases\n",
    "\n",
    "### Bad LLM Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
