{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Interview Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 𝗔𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲 & 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴\n",
    "\n",
    "### Transformer Architecture (attention mechanisms)\n",
    "\n",
    "### Pre-training vs Fine-tuning\n",
    "\n",
    "### Training Objectives (next token prediction)\n",
    "\n",
    "### Context Window and Position Embeddings\n",
    "\n",
    "### Tokenization Strategies\n",
    "\n",
    "### Model Scaling Laws\n",
    "\n",
    "### Parameter Efficient Fine-tuning (LoRA, QLoRA, Prefix Tuning)\n",
    "\n",
    "### Distillation\n",
    "\n",
    "What is knowledge distillation?\n",
    "\n",
    "When did knowledge distillation appear as a technique?\n",
    "* The ideas behind knowledge distillation (KD) date back to 2006, when Bucilă, Caruana, and Niculescu-Mizil in their work “Model Compression” showed that an ensemble of models could be compressed into a single smaller model without much loss in accuracy. They demonstrated that a cumbersome model (like an ensemble) could be effectively replaced by a lean model that was easier to deploy.\n",
    "* Later in 2015, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean coined the term “distillation” in their “Distilling the Knowledge in a Neural Network” paper. This term was referred to the process of transferring knowledge from a large, complex AI model or ensemble to a smaller, faster AI model, called the distilled model​. \n",
    "    * Instead of just training the smaller model on correct answers, researchers proposed to give it the probability distribution from the large model. \n",
    "    * This helps the smaller model learn not just what the right answer is, but also how confident the big model is about each option. This training concept is closely connected to the softmax function.\n",
    "\n",
    "Types of knowledge distillation\n",
    "\n",
    "Improved algorithms\n",
    "\n",
    "Distillation scaling laws\n",
    "\n",
    "Benefits\n",
    "\n",
    "Not without limitations\n",
    "\n",
    "Real-world effective use cases (why OpenAI got mad at DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 𝗖𝗼𝗻𝘁𝗿𝗼𝗹\n",
    "\n",
    "### Temperature and Top-p Sampling\n",
    "\n",
    "### Prompt Engineering Techniques\n",
    "\n",
    "### Few-shot Learning\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "### Chain-of-Thought Prompting\n",
    "\n",
    "### Hallucination Mitigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗟𝗟𝗠 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻 - Metrics \n",
    "\n",
    "### Precision@K\n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It is the ratio of correctly identified relevant items within the total recommended items (the K-long list). As such, it answers the question, out of the top-K items suggested, how many are actually relevant to the user? \n",
    "* K is the cut-off threshold that you choose to limit your evaluation to and it's dependent on the expectation of how many items your user is likely to interact with.\n",
    "* Relevance is a binary label that is specific to your use case. For example, you might measure an item as relevant if a user clicks on it, adds it to a shopping cart, or purchases it.\n",
    "\n",
    "$\\text{Precision@K} = \\frac{\\text{Number of relevant items in K}}{Total number of items in K}$\n",
    "\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### Recall@K \n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It measures the share of revelant items captured within the top K positions\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### F-Score@K\n",
    "\n",
    "A common metric to evaluate the performance of ranking algorithms (search systems, RAG, etc.). It provides a balanced measure of Precision@K and Recall@K\n",
    "* Range: 0-1, a higher value means better performance\n",
    "* Limitations:\n",
    "    * Only reflects the number of relevant items in the top K, doesn't evaluate the ranking quality inside of K\n",
    "\n",
    "### Apology Rate\n",
    "\n",
    "### No Response Rate\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "### ROUGE Scores\n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "### Intelligence\n",
    "\n",
    "### Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LLM Eval Techniques\n",
    "\n",
    "### LLM-as-a-judge\n",
    "\n",
    "### Human Evaluation Methods\n",
    "\n",
    "### Evaluation Data Sets\n",
    "* Benchmark Datasets (MMLU, BigBench, HumanEval)\n",
    "\n",
    "### A/B Testing Between Chatbots\n",
    "\n",
    "### Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗮𝘁𝗶𝗼𝗻 & 𝗗𝗲𝗽𝗹𝗼𝘆𝗺𝗲𝗻𝘁\n",
    "\n",
    "### Quantization Techniques (4-bit, 8-bit)\n",
    "\n",
    "### Model Distillation\n",
    "\n",
    "### Prompt Caching\n",
    "\n",
    "### Model Merging\n",
    "\n",
    "### Inference Optimization\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "### Latency Management\n",
    "\n",
    "### Cost Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 𝗦𝗮𝗳𝗲𝘁𝘆 & 𝗘𝘁𝗵𝗶𝗰𝘀\n",
    "\n",
    "### Content Filtering\n",
    "\n",
    "### Output Sanitization\n",
    "\n",
    "### Jailbreak Prevention\n",
    "\n",
    "### Data Privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Product Sense - Good vs. Bad LLM Use Cases\n",
    "\n",
    "### Good LLM Use Cases\n",
    "\n",
    "### Bad LLM Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
