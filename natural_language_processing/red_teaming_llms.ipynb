{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Teaming LLMs\n",
    "\n",
    "## LLM Jailbreak Mitigation Solutions\n",
    "\n",
    "### Guardrails\n",
    "* This is the general solution to LLM jailbreaks, but they are brittle and become outdated as soon as they're deployed\n",
    "\n",
    "### Consitutional Classifiers\n",
    "* First presented by Anthropic in their paper, \"[Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837).\"\n",
    "* Relies on the premise that AI should guard itself with AI\n",
    "    * Build a \"constitution\" — a set of natural language rules that specifies what’s safe and what’s dangerous. Then, use that constitution to generate synthetic data that trains a new kind of safeguard, called Constitutional Classifiers.\n",
    "    * Then use the trained classifiers to filter both inputs and outputs to stop jailbreaks\n",
    "\n",
    "Solution Architecture (Dual-Layer Defense)\n",
    "* <ins>Input Classifier</ins>: blocks user query attempts to bypass safety filters before they reach the model\n",
    "* <ins>Streaming Output Classifier</ins>: monitors the model's responses to make sure it doesn't return anything harmful\n",
    "\n",
    "---\n",
    "\n",
    "## Jailbreaking Evaluation Frameworks and Red Teaming Attack Strategies\n",
    "\n",
    "### Automated Red Teaming (ART)\n",
    "* <ins>Automated Red Teaming</ins>: Use an LLM to create an AI-driven attack generator that consistently changes its attack strategy based on an evaluation strategy like rubric grading.\n",
    "    * <ins>Rubric grading</ins>: a model evaluates jailbreak attempts based on predefined guidelines.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
